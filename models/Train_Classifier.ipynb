{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install nltk\n",
    "!pip install plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download(['punkt', 'wordnet', 'averaged_perceptron_tagger','stopwords'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import re\n",
    "from sqlalchemy import create_engine\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', 500)\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import fbeta_score, make_scorer\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.multioutput import MultiOutputClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data from database\n",
    "database_filepath = \"../data/disaster_response_db.db\"\n",
    "engine = create_engine('sqlite:///' + database_filepath)\n",
    "table_name = os.path.basename(database_filepath).replace(\".db\",\"\") + \"_table\"\n",
    "df = pd.read_sql_table(table_name,engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/Users/chiguo/DATA_SCIENCE/Proj/Disaster_Response_Pipeline/data/disaster_response_table.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove child alone column as it has all zeros only\n",
    "df = df.drop(['child_alone'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(\"related\").size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['related']==2].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# after reviewing the number of msg and their content for those with related as 2\n",
    "# here we consider them as valid msg and replace 2 with 1 \n",
    "df['related'] = df['related'].apply(lambda x: 1 if x == 2 else x)\n",
    "df.groupby(\"related\").size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract X and y variables from the data for the modelling\n",
    "X = df['message']\n",
    "y = df.iloc[:,4:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a function to normalize, tokenize and lemmatize the text data\n",
    "def tokenize(text, url_place_holder_string=\"urlplaceholder\"):\n",
    "    \"\"\"\n",
    "    Tokenize the text function\n",
    "    \n",
    "    Arguments:\n",
    "        text -> Messages to be tokenized and lemmatized\n",
    "    Output:\n",
    "        clean_tokens -> List of tokens extracted from the provided text\n",
    "    \"\"\"\n",
    "    \n",
    "    # Replace all urls with a urlplaceholder string\n",
    "    url_regex = 'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'\n",
    "    # Extract all the urls from the provided text \n",
    "    detected_urls = re.findall(url_regex, text)\n",
    "    # Replace url with a url placeholder string\n",
    "    for detected_url in detected_urls:\n",
    "        text = text.replace(detected_url, url_place_holder_string)\n",
    "\n",
    "    # remove punctiation\n",
    "    text = re.sub(r'[^a-zA-Z0-9]', \" \", text)\n",
    "    \n",
    "    # Extract the word tokens from the input text\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    \n",
    "    # Remove stop words if any\n",
    "    words = [w for w in tokens if w not in stopwords.words(\"english\")]\n",
    "    \n",
    "    # Lemmatizer to map the words back to its root\n",
    "    lemmatizer = nltk.WordNetLemmatizer()\n",
    "\n",
    "    # List of clean tokens\n",
    "    cleaned_tokens = [lemmatizer.lemmatize(w).lower().strip() for w in words]\n",
    "    return cleaned_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a custom transformer which extract the starting verb of a sentence\n",
    "class StartingVerbExtractor(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Tokenize the text function\n",
    "    \n",
    "    Arguments:\n",
    "        text -> Messages to be tokenized and lemmatized\n",
    "    Output:\n",
    "        cleaned_tokens -> List of tokens extracted from the provided text\n",
    "    \"\"\"\n",
    "    def starting_verb(self, text):\n",
    "        sentence_list = nltk.sent_tokenize(text)\n",
    "        for sentence in sentence_list:\n",
    "            pos_tags = nltk.pos_tag(tokenize(sentence))\n",
    "            first_word, first_tag = pos_tags[0]\n",
    "            if first_tag in ['VB', 'VBP', 'VBD', 'VBG','VBZ', 'VBN'] or first_word == 'RT':\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X_tagged = pd.Series(X).apply(self.starting_verb)\n",
    "        return pd.DataFrame(X_tagged)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a machine learning pipeline\n",
    "\n",
    "pipeline_1 = Pipeline([\n",
    "    ('features', FeatureUnion([\n",
    "\n",
    "        ('text_pipeline', Pipeline([\n",
    "            ('vect', CountVectorizer(tokenizer=tokenize)),\n",
    "            ('tfidf', TfidfTransformer())\n",
    "        ])),\n",
    "\n",
    "        ('starting_verb', StartingVerbExtractor())\n",
    "    ])),\n",
    "\n",
    "    ('clf', MultiOutputClassifier(RandomForestClassifier()))\n",
    "])\n",
    "\n",
    "pipeline_2 = Pipeline([\n",
    "    ('features', FeatureUnion([\n",
    "\n",
    "        ('text_pipeline', Pipeline([\n",
    "            ('vect', CountVectorizer(tokenizer=tokenize)),\n",
    "            ('tfidf', TfidfTransformer())\n",
    "        ])),\n",
    "\n",
    "        ('starting_verb', StartingVerbExtractor())\n",
    "    ])),\n",
    "\n",
    "    ('clf', MultiOutputClassifier(AdaBoostClassifier()))\n",
    "])\n",
    "\n",
    "parameters_1 = {\n",
    "    'clf__estimator__min_samples_split': [2, 4]\n",
    "    #'features__text_pipeline__vect__ngram_range': ((1, 1), (1, 2)),\n",
    "    #'clf__estimator__n_estimators': [50, 100, 200],\n",
    "    #'clf__estimator__min_samples_split': [2, 3, 4]\n",
    "}\n",
    "\n",
    "parameters_2 = {\n",
    "    'clf__estimator__learning_rate': [0.01, 0.02, 0.05],\n",
    "    'clf__estimator__n_estimators': [10, 20, 40]\n",
    "}\n",
    "    \n",
    "grid_1 = GridSearchCV(pipeline_1, param_grid=parameters_1, scoring='f1_micro', n_jobs=-1)\n",
    "grid_2 = GridSearchCV(pipeline_2, param_grid=parameters_2, scoring='f1_micro', n_jobs=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_1.get_params().keys()\n",
    "pipeline_2.get_params().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_1.fit(X_train, y_train)\n",
    "#grid_2.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_1.grid_scores_\n",
    "#grid_2.grid_scores_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#finding the best paramesters based on grip search\n",
    "print(grid_1.best_params_)\n",
    "#print(grid_2.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimised_model_1 = grid_1.best_estimator_\n",
    "#optimised_model_2 = grid_2.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_prediction_train_1 = optimised_model_1.predict(X_train)\n",
    "y_prediction_test_1 = optimised_model_1.predict(X_test)\n",
    "#y_prediction_train_2 = optimised_model_2.predict(X_train)\n",
    "#y_prediction_test_2 = optimised_model_2.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test.values, y_prediction_test_1, target_names=y.columns.values))\n",
    "#print(classification_report(y_train.values, y_prediction_train_1, target_names=y.columns.values))\n",
    "#print(classification_report(y_test.values, y_prediction_test_2, target_names=y.columns.values))\n",
    "#print(classification_report(y_train.values, y_prediction_train_2, target_names=y.columns.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export the trained model as a pickle file\n",
    "with open('clf.pkl', 'wb') as file:\n",
    "    pickle.dump(optimised_model_1, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
